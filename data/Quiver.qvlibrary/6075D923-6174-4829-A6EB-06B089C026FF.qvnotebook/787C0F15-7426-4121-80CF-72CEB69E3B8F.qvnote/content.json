{
  "title": "Untitled Note",
  "cells": [
    {
      "type": "text",
      "data": "<div>目录</div><div>数学符号</div><div>第一章 &nbsp;引言 &nbsp;1</div><div>1.1 &nbsp;本书面向的读者 &nbsp;10</div><div>1.2 &nbsp;深度学习的历史趋势 &nbsp;. &nbsp;11</div><div>1.2.1 &nbsp;神经网络的众多名称和命运变迁 &nbsp;. &nbsp;12</div><div>1.2.2 &nbsp;与日俱增的数据量 &nbsp;. &nbsp;17</div><div>1.2.3 &nbsp;与日俱增的模型规模 &nbsp;19</div><div>1.2.4 &nbsp;与日俱增的精度、复杂度和对现实世界的冲击 &nbsp;22</div><div><br></div><div>第一部分 &nbsp;应用数学与机器学习基础 &nbsp;25</div><div>第二章 &nbsp;线性代数 &nbsp;27 &nbsp;</div><div>2.1 &nbsp;标量、向量、矩阵和张量 &nbsp;27 &nbsp;</div><div>2.2 &nbsp;矩阵和向量相乘 &nbsp;29 &nbsp;</div><div>2.3 &nbsp;单位矩阵和逆矩阵 &nbsp;31 &nbsp;</div><div>2.4 &nbsp;线性相关和生成子空间 &nbsp;32 &nbsp;</div><div>2.5 &nbsp;范数 &nbsp;34 &nbsp;</div><div>2.6 &nbsp;特殊类型的矩阵和向量 &nbsp;36 &nbsp;</div><div>2.7 &nbsp;特征分解 &nbsp;. &nbsp;37</div><div>2.8 &nbsp;奇异值分解 &nbsp;39 &nbsp;</div><div>2.9 &nbsp;Moore-Penrose伪逆 &nbsp;. &nbsp;40 &nbsp;</div><div>2.10 &nbsp;迹运算 &nbsp;. &nbsp;41 &nbsp;</div><div>2.11 &nbsp;行列式 &nbsp;. &nbsp;42 &nbsp;</div><div>2.12 &nbsp;实例:主成分分析 &nbsp;42</div><div><br></div><div>第三章 &nbsp;概率与信息论 &nbsp;47</div><div>3.1 &nbsp;为什么要使用概率? &nbsp;. &nbsp;47</div><div>3.2 &nbsp;随机变量 &nbsp;. &nbsp;49</div><div>3.3 &nbsp;概率分布 &nbsp;. &nbsp;50</div><div>3.3.1 &nbsp;离散型变量和概率质量函数 &nbsp;50</div><div>3.3.2 &nbsp;连续型变量和概率密度函数 &nbsp;51</div><div>3.4 &nbsp;边缘概率 &nbsp;. &nbsp;52</div><div>3.5 &nbsp;条件概率 &nbsp;. &nbsp;52</div><div>3.6 &nbsp;条件概率的链式法则 &nbsp;. &nbsp;53</div><div>3.7 &nbsp;独立性和条件独立性 &nbsp;. &nbsp;53</div><div>3.8 &nbsp;期望、方差和协方差 &nbsp;. &nbsp;54</div><div>3.9 &nbsp;常用概率分布 &nbsp;. &nbsp;55</div><div>3.9.1 &nbsp;Bernoulli分布 &nbsp;. &nbsp;56</div><div>3.9.2 &nbsp;Multinoulli分布 &nbsp;56</div><div>3.9.3 &nbsp;高斯分布 &nbsp;57</div><div>3.9.4 &nbsp;指数分布和Laplace分布 &nbsp;. &nbsp;58</div><div>3.9.5 &nbsp;Dirac分布和经验分布 &nbsp;. &nbsp;59</div><div>3.9.6 &nbsp;分布的混合 &nbsp;. &nbsp;59</div><div>3.10 &nbsp;常用函数的有用性质 &nbsp;. &nbsp;61</div><div>3.11 &nbsp;贝叶斯规则 &nbsp;63</div><div>3.12 &nbsp;连续型变量的技术细节 &nbsp;64</div><div>3.13 &nbsp;信息论 &nbsp;. &nbsp;65</div><div>3.14 &nbsp;结构化概率模型 &nbsp;69</div><div><br></div><div>第四章 &nbsp;数值计算 &nbsp;72</div><div>4.1 &nbsp;上溢和下溢 &nbsp;72 &nbsp;</div><div>4.2 &nbsp;病态条件 &nbsp;. &nbsp;73</div><div>4.3 &nbsp;基于梯度的优化方法 &nbsp;. &nbsp;74 &nbsp;</div><div>4.3.1 &nbsp;梯度之上:Jacobian和Hessian矩阵 &nbsp;. &nbsp;77</div><div>4.4 &nbsp;约束优化 &nbsp;. &nbsp;82</div><div>4.5 &nbsp;实例:线性最小二乘 &nbsp;. &nbsp;85</div><div><br></div><div>第五章 &nbsp;机器学习基础 &nbsp;87</div><div>5.1 &nbsp;学习算法 &nbsp;. &nbsp;87</div><div>5.1.1 &nbsp;任务T &nbsp;. &nbsp;88</div><div>5.1.2 &nbsp;性能度量P &nbsp;. &nbsp;91</div><div>5.1.3 &nbsp;经验E &nbsp;. &nbsp;92</div><div>5.1.4 &nbsp;示例:线性回归 &nbsp;94</div><div>5.2 &nbsp;容量、过拟合和欠拟合 &nbsp;97</div><div>5.2.1 &nbsp;没有免费午餐定理 &nbsp;.102</div><div>5.2.2 &nbsp;正则化 &nbsp;104</div><div>5.3 &nbsp;超参数和验证集 &nbsp;105</div><div>5.3.1 &nbsp;交叉验证 &nbsp;106</div><div>5.4 &nbsp;估计、偏差和方差 &nbsp;108</div><div>5.4.1 &nbsp;点估计 &nbsp;108</div><div>5.4.2 &nbsp;偏差 &nbsp;.109</div><div>5.4.3 &nbsp;方差和标准差 &nbsp;111</div><div>5.4.4 &nbsp;权衡偏差和方差以最小化均方误差 &nbsp;113</div><div>5.4.5 &nbsp;一致性 &nbsp;114</div><div>5.5 &nbsp;最大似然估计 &nbsp;.115</div><div>5.5.1 &nbsp;条件对数似然和均方误差 &nbsp;.116</div><div>5.5.2 &nbsp;最大似然的性质 &nbsp;117</div><div>5.6 &nbsp;贝叶斯统计 &nbsp;118</div><div>5.6.1 &nbsp;最大后验 &nbsp;(MAP) &nbsp;估计 &nbsp;.121</div><div>5.7 &nbsp;监督学习算法 &nbsp;.122</div><div>5.7.1 &nbsp;概率监督学习 &nbsp;122</div><div>5.7.2 &nbsp;支持向量机 &nbsp;.123</div><div>5.7.3 &nbsp;其他简单的监督学习算法 &nbsp;.125</div><div>5.8 &nbsp;无监督学习算法 &nbsp;128</div><div>5.8.1 &nbsp;主成分分析 &nbsp;.128</div><div>5.8.2 &nbsp;k-均值聚类 &nbsp;.131 &nbsp;</div><div>5.9 &nbsp;随机梯度下降 &nbsp;.132 &nbsp;</div><div>5.10 &nbsp;构建机器学习算法 &nbsp;133 &nbsp;</div><div>5.11 &nbsp;促使深度学习发展的挑战 &nbsp;134</div><div>5.11.1 &nbsp;维数灾难 &nbsp;135 &nbsp;</div><div>5.11.2 &nbsp;局部不变性和平滑正则化 &nbsp;.135 &nbsp;</div><div>5.11.3 &nbsp;流形学习 &nbsp;139</div><div><br></div><div>第二部分 &nbsp;深度网络:现代实践 &nbsp;143</div><div>第六章 &nbsp;深度前馈网络 &nbsp;145</div><div>6.1 &nbsp;实例:学习XOR &nbsp;.148</div><div>6.2 &nbsp;基于梯度的学习 &nbsp;152</div><div>6.2.1 &nbsp;代价函数 &nbsp;153 &nbsp;</div><div>6.2.1.1 &nbsp;使用最大似然学习条件分布 &nbsp;.154 &nbsp;</div><div>6.2.1.2 &nbsp;学习条件统计量 &nbsp;155</div><div>6.2.2 &nbsp;输出单元 &nbsp;156 &nbsp;</div><div>6.2.2.1 &nbsp;用于高斯输出分布的线性单元 &nbsp;156 &nbsp;</div><div>6.2.2.2 &nbsp;用于 &nbsp;Bernoulli &nbsp;输出分布的 &nbsp;sigmoid &nbsp;单元 &nbsp;157 &nbsp;</div><div>6.2.2.3 &nbsp;用于 &nbsp;Multinoulli &nbsp;输出分布的 &nbsp;softmax &nbsp;单元 &nbsp;.159 &nbsp;</div><div>6.2.2.4 &nbsp;其他的输出类型 &nbsp;162</div><div>6.3 &nbsp;隐藏单元 &nbsp;.165</div><div>6.3.1 &nbsp;整流线性单元及其扩展 &nbsp;.166</div><div>6.3.2 &nbsp;logisticsigmoid与双曲正切函数 &nbsp;.168</div><div>6.3.3 &nbsp;其他隐藏单元 &nbsp;169</div><div>6.4 &nbsp;架构设计 &nbsp;.170</div><div>6.4.1 &nbsp;万能近似性质和深度 &nbsp;171</div><div>6.4.2 &nbsp;其他架构上的考虑 &nbsp;.174</div><div>6.5 &nbsp;反向传播和其他的微分算法 &nbsp;.175</div><div>6.5.1 &nbsp;计算图 &nbsp;176</div><div>6.5.2 &nbsp;微积分中的链式法则 &nbsp;178</div><div>6.5.3 &nbsp;递归地使用链式法则来实现反向传播 &nbsp;.179</div><div>6.6</div><div>6.5.4 &nbsp;全连接MLP中的反向传播计算 &nbsp;181</div><div>6.5.5 &nbsp;符号到符号的导数 &nbsp;.182</div><div>6.5.6 &nbsp;一般化的反向传播 &nbsp;.185</div><div>6.5.7 &nbsp;实例:用于MLP训练的反向传播 &nbsp;188</div><div>6.5.8 &nbsp;复杂化 &nbsp;190</div><div>6.5.9 &nbsp;深度学习界以外的微分 &nbsp;.191</div><div>6.5.10 &nbsp;高阶微分 &nbsp;193</div><div>历史小记 &nbsp;.193</div><div><br></div><div>第七章 &nbsp;深度学习中的正则化 &nbsp;197</div><div>7.1 &nbsp;参数范数惩罚 &nbsp;.198</div><div>7.1.1 &nbsp;L2参数正则化 &nbsp;.199</div><div>7.1.2 &nbsp;L1参数正则化 &nbsp;.202</div><div>7.2 &nbsp;作为约束的范数惩罚 &nbsp;.204</div><div>7.3 &nbsp;正则化和欠约束问题 &nbsp;.206</div><div>7.4 &nbsp;数据集增强 &nbsp;207</div><div>7.5 &nbsp;噪声鲁棒性 &nbsp;208</div><div>7.5.1 &nbsp;向输出目标注入噪声 &nbsp;209</div><div>7.6 &nbsp;半监督学习 &nbsp;209</div><div>7.7 &nbsp;多任务学习 &nbsp;210</div><div>7.8 &nbsp;提前终止 &nbsp;.211</div><div>7.9 &nbsp;参数绑定和参数共享 &nbsp;.217</div><div>7.9.1 &nbsp;卷积神经网络 &nbsp;218</div><div>7.10 &nbsp;稀疏表示 &nbsp;.218</div><div>7.11 &nbsp;Bagging和其他集成方法 &nbsp;.220</div><div>7.12 &nbsp;Dropout &nbsp;222</div><div>7.13 &nbsp;对抗训练 &nbsp;.230</div><div>7.14 &nbsp;切面距离、正切传播和流形正切分类器 &nbsp;.232</div><div><br></div><div>第八章 &nbsp;深度模型中的优化 &nbsp;235 &nbsp;</div><div>8.1 &nbsp;学习和纯优化有什么不同 &nbsp;235</div><div>8.1.1 &nbsp;经验风险最小化 &nbsp;236</div><div>8.1.2 &nbsp;代理损失函数和提前终止 &nbsp;.237</div><div>8.1.3 &nbsp;批量算法和小批量算法 &nbsp;.237 &nbsp;</div><div>8.2 &nbsp;神经网络优化中的挑战 &nbsp;241</div><div>8.2.1 &nbsp;病态 &nbsp;.242</div><div>8.2.2 &nbsp;局部极小值 &nbsp;.243</div><div>8.2.3 &nbsp;高原、鞍点和其他平坦区域 &nbsp;244</div><div>8.2.4 &nbsp;悬崖和梯度爆炸 &nbsp;246</div><div>8.2.5 &nbsp;长期依赖 &nbsp;247</div><div>8.2.6 &nbsp;非精确梯度 &nbsp;.248</div><div>8.2.7 &nbsp;局部和全局结构间的弱对应 &nbsp;248</div><div>8.2.8 &nbsp;优化的理论限制 &nbsp;250</div><div><br></div><div>8.3 &nbsp;基本算法 &nbsp;.251</div><div>8.3.1 &nbsp;随机梯度下降 &nbsp;.251</div><div>8.3.2 &nbsp;动量 &nbsp;.253</div><div>8.3.3 &nbsp;Nesterov &nbsp;动量 &nbsp;.256</div><div>8.4 &nbsp;参数初始化策略 &nbsp;.256 &nbsp;</div><div>8.5 &nbsp;自适应学习率算法 &nbsp;261</div><div>8.5.1 &nbsp;AdaGrad &nbsp;261</div><div>8.5.2 &nbsp;RMSProp &nbsp;262</div><div>8.5.3 &nbsp;Adam &nbsp;262</div><div>8.5.4 &nbsp;选择正确的优化算法 &nbsp;263</div><div>8.6 &nbsp;二阶近似方法 &nbsp;.265</div><div>8.6.1 &nbsp;牛顿法 &nbsp;266</div><div>8.6.2 &nbsp;共轭梯度 &nbsp;267</div><div>8.6.3 &nbsp;BFGS &nbsp;270</div><div>8.7 &nbsp;优化策略和元算法 &nbsp;271</div><div>8.7.1 &nbsp;批标准化 &nbsp;271</div><div>8.7.2 &nbsp;坐标下降 &nbsp;274</div><div>8.7.3 &nbsp;Polyak平均 &nbsp;.274</div><div>8.7.4 &nbsp;监督预训练 &nbsp;.275</div><div>8.7.5 &nbsp;设计有助于优化的模型 &nbsp;.277</div><div>8.7.6 &nbsp;延拓法和课程学习 &nbsp;.278</div><div>&nbsp;&nbsp;</div><div>第九章 &nbsp;卷积网络 &nbsp;281</div><div>9.1 &nbsp;卷积运算 &nbsp;.282</div><div>9.2 &nbsp;动机 &nbsp;285</div><div>9.3 &nbsp;池化 &nbsp;290</div><div>9.4 &nbsp;卷积与池化作为一种无限强的先验 &nbsp;.295</div><div>9.5 &nbsp;基本卷积函数的变体 &nbsp;.296</div><div>9.6 &nbsp;结构化输出 &nbsp;306</div><div>9.7 &nbsp;数据类型 &nbsp;.307</div><div>9.8 &nbsp;高效的卷积算法 &nbsp;309</div><div>9.9 &nbsp;随机或无监督的特征 &nbsp;.310</div><div>9.10 &nbsp;卷积网络的神经科学基础 &nbsp;311</div><div>9.11 &nbsp;卷积网络与深度学习的历史 &nbsp;.317</div><div><br></div><div>第十章 &nbsp;序列建模:循环和递归网络 &nbsp;319</div><div>10.1 &nbsp;展开计算图 &nbsp;320</div><div>10.2 &nbsp;循环神经网络 &nbsp;.323</div><div>10.2.1 &nbsp;导师驱动过程和输出循环网络 &nbsp;.326</div><div>10.2.2 &nbsp;计算循环神经网络的梯度 &nbsp;.328</div><div>10.2.3 &nbsp;作为有向图模型的循环网络 &nbsp;330</div><div>10.2.4 &nbsp;基于上下文的RNN序列建模 &nbsp;.334</div><div>10.3 &nbsp;双向RNN &nbsp;.336</div><div>10.4 &nbsp;基于编码-解码的序列到序列架构 &nbsp;338</div><div>10.5 &nbsp;深度循环网络 &nbsp;.340</div><div>10.6 &nbsp;递归神经网络 &nbsp;.341</div><div>10.7 &nbsp;长期依赖的挑战 &nbsp;343</div><div>10.8 &nbsp;回声状态网络 &nbsp;.345</div><div>10.9 &nbsp;渗漏单元和其他多时间尺度的策略 &nbsp;.347</div><div>10.9.1 &nbsp;时间维度的跳跃连接 &nbsp;347</div><div>10.9.2 &nbsp;渗漏单元和一系列不同时间尺度 &nbsp;.348</div><div>10.9.3 &nbsp;删除连接 &nbsp;348</div><div>10.10 &nbsp;长短期记忆和其他门控RNN &nbsp;349 &nbsp;</div><div>10.10.1 &nbsp;LSTM &nbsp;349 &nbsp;</div><div>10.10.2 &nbsp;其他门控RNN &nbsp;.351</div><div>10.11 &nbsp;优化长期依赖 &nbsp;.352 &nbsp;</div><div>10.11.1 &nbsp;截断梯度 &nbsp;353 &nbsp;</div><div>10.11.2 &nbsp;引导信息流的正则化 &nbsp;355</div><div>10.12外显记忆 &nbsp;.355 &nbsp;</div><div><br></div><div>第十一章 &nbsp;实践方法论 &nbsp;359</div><div>11.1 &nbsp;性能度量 &nbsp;.360</div><div>11.2 &nbsp;默认的基准模型 &nbsp;362</div><div>11.3 &nbsp;决定是否收集更多数据 &nbsp;363 &nbsp;</div><div>11.4 &nbsp;选择超参数 &nbsp;364</div><div>11.4.1 &nbsp;手动调整超参数 &nbsp;364</div><div>11.4.2 &nbsp;自动超参数优化算法 &nbsp;367</div><div>11.4.3 &nbsp;网格搜索 &nbsp;368</div><div>11.4.4 &nbsp;随机搜索 &nbsp;369</div><div>11.4.5 &nbsp;基于模型的超参数优化 &nbsp;.370</div><div>11.5 &nbsp;调试策略 &nbsp;.371</div><div>11.6 &nbsp;示例:多位数字识别 &nbsp;.374</div><div><br></div><div>第十二章 &nbsp;应用 &nbsp;377</div><div>12.1 &nbsp;大规模深度学习 &nbsp;377</div><div>12.1.1 &nbsp;快速的CPU实现 &nbsp;.378</div><div>12.1.2 &nbsp;GPU实现 &nbsp;378</div><div>12.1.3 &nbsp;大规模的分布式实现 &nbsp;380</div><div>12.1.4 &nbsp;模型压缩 &nbsp;381</div><div>12.1.5 &nbsp;动态结构 &nbsp;382</div><div>12.1.6 &nbsp;深度网络的专用硬件实现 &nbsp;.384</div><div>12.2 &nbsp;计算机视觉 &nbsp;385 &nbsp;</div><div>12.2.1 &nbsp;预处理 &nbsp;385</div><div>12.2.1.1对比度归一化 &nbsp;.386 &nbsp;</div><div>12.2.2 &nbsp;数据集增强 &nbsp;.389 &nbsp;</div><div>12.3 &nbsp;语音识别 &nbsp;.390 &nbsp;</div><div>12.4 &nbsp;自然语言处理 &nbsp;.392 &nbsp;</div><div>12.4.1 &nbsp;n-gram &nbsp;.392</div><div>12.4.2 &nbsp;神经语言模型 &nbsp;394 &nbsp;</div><div>12.4.3 &nbsp;高维输出 &nbsp;396 &nbsp;</div><div>12.4.3.1使用短列表 &nbsp;396 &nbsp;</div><div>12.4.3.2分层Softmax &nbsp;.397 &nbsp;</div><div>12.4.3.3 &nbsp;重要采样 &nbsp;399 &nbsp;</div><div>12.4.3.4 &nbsp;噪声对比估计和排名损失 &nbsp;.401</div><div>12.4.4 &nbsp;结合 &nbsp;n-gram &nbsp;和神经语言模型 &nbsp;.401</div><div>12.4.5 &nbsp;神经机器翻译 &nbsp;402</div><div>12.4.5.1 &nbsp;使用注意力机制并对齐数据片段 &nbsp;.403 &nbsp;</div><div>12.4.6 &nbsp;历史展望 &nbsp;406</div><div>12.5 &nbsp;其他应用 &nbsp;.407 &nbsp;</div><div>12.5.1 &nbsp;推荐系统 &nbsp;407 &nbsp;</div><div>12.5.1.1探索与利用 &nbsp;409 &nbsp;</div><div>12.5.2 &nbsp;知识表示、推理和回答 &nbsp;.410 &nbsp;</div><div>12.5.2.1 &nbsp;知识、联系和回答 &nbsp;.410</div><div><br></div><div>第三部分 &nbsp;深度学习研究 &nbsp;414</div><div>第十三章 &nbsp;线性因子模型 &nbsp;417 &nbsp;</div><div>13.1 &nbsp;概率PCA和因子分析 &nbsp;418 &nbsp;</div><div>13.2 &nbsp;独立成分分析 &nbsp;.419 &nbsp;</div><div>13.3 &nbsp;慢特征分析 &nbsp;421 &nbsp;</div><div>13.4 &nbsp;稀疏编码 &nbsp;.423 &nbsp;</div><div>13.5 &nbsp;PCA的流形解释 &nbsp;.426</div><div><br></div><div>第十四章 &nbsp;自编码器 &nbsp;429</div><div>14.1 &nbsp;欠完备自编码器 &nbsp;430</div><div>14.2 &nbsp;正则自编码器 &nbsp;.431</div><div>14.2.1 &nbsp;稀疏自编码器 &nbsp;431</div><div>14.2.2 &nbsp;去噪自编码器 &nbsp;433</div><div>14.2.3 &nbsp;惩罚导数作为正则 &nbsp;.434</div><div>14.3 &nbsp;表示能力、层的大小和深度 &nbsp;.434</div><div>14.4 &nbsp;随机编码器和解码器 &nbsp;.435</div><div>14.5 &nbsp;去噪自编码器 &nbsp;.436 &nbsp;</div><div>14.5.1 &nbsp;得分估计 &nbsp;437 &nbsp;</div><div>14.5.2 &nbsp;历史展望 &nbsp;440 &nbsp;</div><div>14.6 &nbsp;使用自编码器学习流形 &nbsp;440 &nbsp;</div><div>14.7 &nbsp;收缩自编码器 &nbsp;.445 &nbsp;</div><div>14.8 &nbsp;预测稀疏分解 &nbsp;.447 &nbsp;</div><div>14.9 &nbsp;自编码器的应用 &nbsp;448</div><div><br></div><div>第十五章 &nbsp;表示学习 &nbsp;449 &nbsp;</div><div>15.1 &nbsp;贪心逐层无监督预训练 &nbsp;450 &nbsp;</div><div>15.1.1 &nbsp;何时以及为何无监督预训练有效? &nbsp;452</div><div>15.2 &nbsp;迁移学习和领域自适应 &nbsp;457</div><div>15.3 &nbsp;半监督解释因果关系 &nbsp;.461</div><div>15.4 &nbsp;分布式表示 &nbsp;466</div><div>15.5 &nbsp;得益于深度的指数增益 &nbsp;471</div><div>15.6 &nbsp;提供发现潜在原因的线索 &nbsp;472</div><div><br></div><div>第十六章 &nbsp;深度学习中的结构化概率模型 &nbsp;475</div><div>16.1 &nbsp;非结构化建模的挑战 &nbsp;.476</div><div>16.2 &nbsp;使用图描述模型结构 &nbsp;.479</div><div>16.2.1 &nbsp;有向模型 &nbsp;480</div><div>16.2.2 &nbsp;无向模型 &nbsp;482</div><div>16.2.3 &nbsp;配分函数 &nbsp;484</div><div>16.2.4 &nbsp;基于能量的模型 &nbsp;485</div><div>16.2.5 &nbsp;分离和d-分离 &nbsp;.487</div><div>16.2.6 &nbsp;在有向模型和无向模型中转换 &nbsp;.490</div><div>16.2.7 &nbsp;因子图 &nbsp;493</div><div>16.3 &nbsp;从图模型中采样 &nbsp;494</div><div>16.4 &nbsp;结构化建模的优势 &nbsp;495</div><div>16.5 &nbsp;学习依赖关系 &nbsp;.496</div><div>16.6 &nbsp;推断和近似推断 &nbsp;497</div><div>16.7 &nbsp;结构化概率模型的深度学习方法 &nbsp;.498</div><div>16.7.1 &nbsp;实例:受限玻尔兹曼机 &nbsp;.499</div><div><br></div><div>第十七章 &nbsp;蒙特卡罗方法 &nbsp;502 &nbsp;</div><div>17.1 &nbsp;采样和蒙特卡罗方法 &nbsp;.502 &nbsp;</div><div>17.1.1 &nbsp;为什么需要采样? &nbsp;.502 &nbsp;</div><div>17.1.2 &nbsp;蒙特卡罗采样的基础 &nbsp;503</div><div>17.2 &nbsp;重要采样 &nbsp;.504</div><div>17.3 &nbsp;马尔可夫链蒙特卡罗方法 &nbsp;506</div><div>17.4 &nbsp;Gibbs采样 &nbsp;510</div><div>17.5 &nbsp;不同的峰值之间的混合挑战 &nbsp;.511</div><div>17.5.1 &nbsp;不同峰值之间通过回火来混合 &nbsp;.513 &nbsp;</div><div>17.5.2 &nbsp;深度也许会有助于混合 &nbsp;.514</div><div><br></div><div>第十八章 &nbsp;直面配分函数 &nbsp;516</div><div>18.1 &nbsp;对数似然梯度 &nbsp;.516</div><div>18.2 &nbsp;随机最大似然和对比散度 &nbsp;518</div><div>18.3 &nbsp;伪似然 &nbsp;.524</div><div>18.4 &nbsp;得分匹配和比率匹配 &nbsp;.526</div><div>18.5 &nbsp;去噪得分匹配 &nbsp;.528</div><div>18.6 &nbsp;噪声对比估计 &nbsp;.529</div><div>18.7 &nbsp;估计配分函数 &nbsp;.531</div><div>18.7.1 &nbsp;退火重要采样 &nbsp;533 &nbsp;</div><div>18.7.2 &nbsp;桥式采样 &nbsp;536</div><div><br></div><div>第十九章 &nbsp;近似推断 &nbsp;538</div><div>19.1 &nbsp;把推断视作优化问题 &nbsp;.539</div><div>19.2 &nbsp;期望最大化 &nbsp;541</div><div>19.3 &nbsp;最大后验推断和稀疏编码 &nbsp;542</div><div>19.4 &nbsp;变分推断和变分学习 &nbsp;.544</div><div>19.4.1 &nbsp;离散型潜变量 &nbsp;545</div><div>19.4.2 &nbsp;变分法 &nbsp;551</div><div>19.4.3 &nbsp;连续型潜变量 &nbsp;554</div><div>19.4.4 &nbsp;学习和推断之间的相互作用 &nbsp;556</div><div>19.5 &nbsp;学成近似推断 &nbsp;.556 &nbsp;</div><div>19.5.1 &nbsp;醒眠算法 &nbsp;557</div><div>19.5.2 &nbsp;学成推断的其他形式 &nbsp;557</div><div>&nbsp;&nbsp;</div><div>第二十章 &nbsp;深度生成模型 &nbsp;559</div><div>20.1 &nbsp;玻尔兹曼机 &nbsp;559</div><div>20.2 &nbsp;受限玻尔兹曼机 &nbsp;561 &nbsp;</div><div>20.2.1 &nbsp;条件分布 &nbsp;562</div><div>20.2.2 &nbsp;训练受限玻尔兹曼机 &nbsp;563</div><div>20.3 &nbsp;深度信念网络 &nbsp;.564</div><div>20.4 &nbsp;深度玻尔兹曼机 &nbsp;566</div><div>20.4.1 &nbsp;有趣的性质 &nbsp;.568</div><div>20.4.2 &nbsp;DBM均匀场推断 &nbsp;.569</div><div>20.4.3 &nbsp;DBM的参数学习 &nbsp;.571</div><div>20.4.4 &nbsp;逐层预训练 &nbsp;.572</div><div>20.4.5 &nbsp;联合训练深度玻尔兹曼机 &nbsp;.574</div><div>20.5 &nbsp;实值数据上的玻尔兹曼机 &nbsp;578 &nbsp;</div><div>20.5.1 &nbsp;Gaussian-BernoulliRBM &nbsp;.578</div><div>20.5.2 &nbsp;条件协方差的无向模型 &nbsp;.579</div><div>20.6 &nbsp;卷积玻尔兹曼机 &nbsp;583</div><div>20.7 &nbsp;用于结构化或序列输出的玻尔兹曼机 &nbsp;585</div><div>20.8 &nbsp;其他玻尔兹曼机 &nbsp;586</div><div>20.9 &nbsp;通过随机操作的反向传播 &nbsp;587</div><div>20.9.1 &nbsp;通过离散随机操作的反向传播 &nbsp;.588 &nbsp;</div><div>20.10 &nbsp;有向生成网络 &nbsp;.591 &nbsp;</div><div>20.10.1 &nbsp;sigmoid信念网络 &nbsp;591 &nbsp;</div><div>20.10.2 &nbsp;可微生成器网络 &nbsp;592 &nbsp;</div><div>20.10.3 &nbsp;变分自编码器 &nbsp;594 &nbsp;</div><div>20.10.4 &nbsp;生成式对抗网络 &nbsp;597 &nbsp;</div><div>20.10.5 &nbsp;生成矩匹配网络 &nbsp;600 &nbsp;</div><div>20.10.6 &nbsp;卷积生成网络 &nbsp;601 &nbsp;</div><div>20.10.7 &nbsp;自回归网络 &nbsp;.602 &nbsp;</div><div>20.10.8 &nbsp;线性自回归网络 &nbsp;602 &nbsp;</div><div>20.10.9 &nbsp;神经自回归网络 &nbsp;603 &nbsp;</div><div>20.10.10 &nbsp;NADE &nbsp;604</div><div><br></div>"
    }
  ]
}