{
  "title": "计算文本相似度",
  "cells": [
    {
      "type": "text",
      "data": "<div style=\"word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; white-space: normal;\"><div style=\"margin: 0px 0px 10px; padding: 0px; font-family: arial, &amp;apos;courier new&amp;apos;, courier, 宋体, monospace, &amp;apos;Microsoft YaHei&amp;apos;; font-size: 14px; color: rgb(51, 51, 51); font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; widows: 1; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(243, 255, 236);\"><div><br/></div><div>最近正好组内做了一个文档相似度的分享。决定回答一发。</div>\n&nbsp; 首先，如果不局限于NN的方法，可以用BOW+<a href=\"https://www.baidu.com/s?wd=tf-idf&amp;amp;tn=44039180_cpr&amp;amp;fenlei=mv6quAkxTZn0IZRqIHckPjm4nH00T1dBmW7hPWuBPhDduj79PHTs0ZwV5Hcvrjm3rH6sPfKWUMw85HfYnjn4nH6sgvPsT6KdThsqpZwYTjCEQLGCpyw9Uz4Bmy-bIi4WUvYETgN-TLwGUv3EnW0LPHRzPWbknHnYP16vP1D3r0\" rel=\"nofollow\" style=\"color: rgb(45, 100, 179); text-decoration: none;\" target=\"_blank\">tf-idf</a>+LSI/LDA的体系搞定，也就是俗称的01或one&nbsp;hot&nbsp;representation。<br/>\n&nbsp; 其次，如果楼主指定了必须用流行的NN，俗称word-embedding的方法，当然首推word2vec（虽然不算是DNN）。然后得到了word2vec的词向量后，可以通过简单加权/tag加权/<a href=\"https://www.baidu.com/s?wd=tf-idf&amp;amp;tn=44039180_cpr&amp;amp;fenlei=mv6quAkxTZn0IZRqIHckPjm4nH00T1dBmW7hPWuBPhDduj79PHTs0ZwV5Hcvrjm3rH6sPfKWUMw85HfYnjn4nH6sgvPsT6KdThsqpZwYTjCEQLGCpyw9Uz4Bmy-bIi4WUvYETgN-TLwGUv3EnW0LPHRzPWbknHnYP16vP1D3r0\" rel=\"nofollow\" style=\"color: rgb(45, 100, 179); text-decoration: none;\" target=\"_blank\">tf-idf</a>加权等方式得到文档向量。这算是一种方法。当然，加权之前一般应该先干掉stop&nbsp;word，词聚类处理一下。<br/>\n&nbsp; 还有，doc2vec中的paragraph&nbsp;vector也属于直接得到doc向量的方法。特点就是修改了word2vec中的cbow和skip-gram模型。依据论文《Distributed&nbsp;Representations&nbsp;of&nbsp;Sentences&nbsp;and&nbsp;Documents》(ICML&nbsp;2014)。<br/>\n&nbsp; 还有一种根据句法树加权的方式，是ICML2011提出的，见论文《Parsing&nbsp;Natural&nbsp;Scenes&nbsp;and&nbsp;Natural&nbsp;Language&nbsp;with&nbsp;Recursive&nbsp;Neural&nbsp;Networks》，后续也有多个改编的版本。<br/>\n&nbsp; 当然，得到词向量的方式不局限于word2vec，RNNLM和glove也能得到传说中高质量的词向量。<br/>\n&nbsp; ICML2015的论文《From&nbsp;Word&nbsp;Embeddings&nbsp;To&nbsp;Document&nbsp;Distances,&nbsp;Kusner,&nbsp;Washington&nbsp;University》新提出一种计算doc相似度的方式，大致思路是将词之间的余弦距离作为ground&nbsp;distance，词频作为权重，在权重的约束条件下，求WMD的线性规划最优解。<br/>\n&nbsp; 最后，kaggle101中的一个word2vec题目的tutorial里作者如是说：他试了一下简单加权和各种加权，不管如何处理，效果还不如01，归其原因作者认为加权的方式丢失了最重要的句子结构信息（也可以说是词序信息），而doc2vec的方法则保存了这种信息。<br/>\n&nbsp; 在刚刚结束的ACL2015上，似乎很多人提到了glove的方法，其思想是挖掘词共现信息的内在含义，据说是基于全局统计的方法（LSI为代表）与基于局部预测的方法（word2vec为代表）的折衷，而且输出的词向量在词聚类任务上干掉了word2vec的结果，也可以看看。《GloVe:&nbsp;Global&nbsp;Vectors&nbsp;forWord&nbsp;Representation》<br/></div></div>"
    }
  ]
}